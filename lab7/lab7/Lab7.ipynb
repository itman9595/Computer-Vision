{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern recognition: Lab 7\n",
    "### Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid\n",
    "* Add moments rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x*(1.0-sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [  7.15522022e-06]\n",
      "[0 1] [ 0.99663053]\n",
      "[1 0] [ 0.99657377]\n",
      "[1 1] [-0.00204048]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.28642964, -0.80010289,  0.40690119],\n",
      "       [-0.20457916, -0.77214733, -0.62951912],\n",
      "       [ 0.47553069, -0.7816905 , -0.71294366]]), array([[-0.19336937],\n",
      "       [ 0.64556221],\n",
      "       [-0.84146864]])]\n",
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRBL6/y6U/Ap1jtqVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c2b2278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [ 0.01533902]\n",
      "[0 1] [ 0.98034317]\n",
      "[1 0] [ 0.98621667]\n",
      "[1 1] [ 0.02118528]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x*(1.0-x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - (x)**2\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.errors = []\n",
    "        self.activation = sigmoid\n",
    "        self.activation_prime = sigmoid_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "           \n",
    "            # output layer - random((2+1, 1)) : 3 x 1\n",
    "            r = 2*np.random.random( (layers[i]+1 , layers[i+1])) -1\n",
    "            self.weights.append(r)\n",
    "        print(self.weights) \n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000 ,momentum = 0.4):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        errors=[]\n",
    "        average = []\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        \n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            \n",
    "            a = [X[i]]\n",
    "            \n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    \n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            #self.errors.append(error**2)/(k)\n",
    "            errors.append((error**2))\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "            #plt.plot(deltas)\n",
    "            #plt.axis()\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            pr_weights_delta = []\n",
    "            pr_3_1 = np.zeros((3,1))\n",
    "            pr_3_3 = np.zeros((3,3))\n",
    "            pr_weights_delta.append(pr_3_3)\n",
    "            pr_weights_delta.append(pr_3_1)\n",
    "            \n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                delta = learning_rate * (layer.T.dot(delta)-(momentum * pr_weights_delta[i]))\n",
    "                self.weights[i] += delta\n",
    "                self.weights[i] += momentum * pr_weights_delta[i]\n",
    "                pr_weights_delta[i] = delta\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "                t = np.average(errors)\n",
    "                average.append(t)\n",
    "        plt.plot(average)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('error')\n",
    "        plt.show()       \n",
    "        #plt.plot(delta)\n",
    "        #plt.show()\n",
    "            \n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "#          print(a)\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
